# models.py
import logging
import os
import sys
from abc import ABC, abstractmethod

import requests
import torch
import yaml
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain_openai.chat_models import AzureChatOpenAI
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    GenerationConfig,
)

import chainlit as cl


@cl.step
def tool():
    return "Response from the tool!"


class BaseModel(ABC):
    @abstractmethod
    def get_response(self, prompt: str):
        raise NotImplementedError("This method should be overridden by subclass")


class ChatBasedGPT4Model(BaseModel):
    @staticmethod
    def get_response(input_dict):
        try:
            with open("./backend/config.yaml", "r") as f:
                config = yaml.safe_load(f)
        except Exception as e:
            print(f"Failed to load config: {e}")

        llm = AzureChatOpenAI(
            # openai_api_base=config["OPENAI_API_BASE"],
            openai_api_version=config["OPENAI_API_VERSION"],
            deployment_name="gpt-4",
            openai_api_key=config["OPENAI_API_KEY"],
            openai_api_type=config["OPENAI_API_TYPE"],
            azure_endpoint=config["AZURE_ENDPOINT"],
        )

        system_message = SystemMessage(content=input_dict["system_prompt"])
        # AIMessage(content=input_prompt_template.GPT_ASSISTANT_PROMPT_TEMPLATE.format(input=""))
        user_message = HumanMessage(content=input_dict["context_prompt"])

        response = llm(messages=[system_message, user_message])
        if not response.content:
            print("No content in the response")
            return None
        result = response.content
        # print(f"result : {result}")

        return result


@cl.on_message  # this function will be called every time a user inputs a message in the UI
async def main(message: cl.Message):
    """
    This function is called every time a user inputs a message in the UI.
    It sends back an intermediate response from the tool, followed by the final answer.

    Args:
        message: The user's message.

    Returns:
        None.
    """

    # Initialize the ChatBasedGPT4Model
    gpt4_model = ChatBasedGPT4Model()

    # ë©”ì‹œì§€ ë‚´ìš©ì— ë”°ë¼ ì²˜ë¦¬ ë°©ì‹ì„ ë¶„ê¸°
    if "ğŸ¤–" in message.content:
        input_dict = {
            "system_prompt": "ì´ëª¨í‹°ì½˜ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•´ì„œ ì§ˆë¬¸ì— ì˜¬ë°”ë¥´ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.",
            "context_prompt": message.content[1:],  # ì´ëª¨ì§€ string ì œì™¸
        }
    else:  # ì¼ë°˜ì ì¸ ê²½ìš°ì˜ ì²˜ë¦¬
        input_dict = {
            "system_prompt": "Please provide a response to the following",  # ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°ì •
            "context_prompt": message.content,  # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
        }

    # GPT-4 ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µ ë°›ê¸°
    gpt4_response = gpt4_model.get_response(input_dict)

    # GPT-4 ì‘ë‹µì´ ìˆëŠ”ì§€ í™•ì¸
    if gpt4_response:
        # GPT-4 ì‘ë‹µì„ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë³´ë‚´ê¸°
        await cl.Message(content=gpt4_response).send()
    else:
        # GPT-4ë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í•œ ê²½ìš°, ëŒ€ì²´ ë©”ì‹œì§€ ë³´ë‚´ê¸°
        await cl.Message(
            content="Sorry, I couldn't get a response. Please try again."
        ).send()
